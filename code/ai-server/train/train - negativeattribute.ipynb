{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "329b3cd9-a9a3-47b5-b3a8-50acc0041829",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorWithPadding,EarlyStoppingCallback\n",
    "from peft import get_peft_model,get_peft_config, LoraConfig, TaskType, prepare_model_for_kbit_training\n",
    "from datasets import Dataset, DatasetDict\n",
    "import pandas as pd\n",
    "import torch\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from trl import SFTTrainer\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9161cc86-1caa-4e14-90a4-1e46fe1439ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN = \"{Enter token here}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d74307fc-07a3-4aee-8891-e456c19bfe38",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3B-instruct\", padding_side=\"right\", token=TOKEN,)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b15b012-db79-4154-a416-ed452e8c4300",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['lm_int8_enable_fp32_cpu_offload']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
     ]
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    lm_int8_enable_fp32_cpu_offload=True,\n",
    "    llm_int8_skip_modules=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9197cb5-e431-436c-ae04-9d5852432f21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78ce836de79a48eb9d4802b2b781a9dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-3.2-3B-instruct\",\n",
    "    quantization_config=bnb_config,\n",
    "    token=TOKEN,\n",
    "    device_map='auto',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "058e3a06-87ea-4958-9785-65739590485b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8184f722-9f90-4e98-93c5-b139018e9dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 16,509,440 || all params: 3,229,259,264 || trainable%: 0.5112\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jasonspc/course-assn/cs3450/cs3450-team-1-project/ai-backend/train/train-env/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:500: UserWarning: Model with `tie_word_embeddings=True` and the tied_target_modules=['lm_head'] are part of the adapter. This can lead to complications, for example when merging the adapter or converting your model to formats other than safetensors. See for example https://github.com/huggingface/peft/issues/2018.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "target_modules = ['q_proj','k_proj','v_proj','o_proj','gate_proj','down_proj','up_proj','lm_head']\n",
    "# PEFT Configuration\n",
    "peft_config = LoraConfig(\n",
    "    r=10,\n",
    "    target_modules = target_modules,\n",
    "    lora_alpha=8,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)\n",
    "print(model.print_trainable_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bd0cb9d-c9d0-42fa-a3dc-95059eda6566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare datasets\n",
    "df1 = pd.read_csv(\"dataset/dataset-checkpoint-1-processed.csv\")\n",
    "df2 = pd.read_csv(\"dataset/dataset-checkpoint-2-processed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfb15e1d-6798-4a59-8f60-da518e8277af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>comment</th>\n",
       "      <th>quality</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>positive-attribute</th>\n",
       "      <th>negative-attribute</th>\n",
       "      <th>processed_comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>544</th>\n",
       "      <td>103621.0</td>\n",
       "      <td>Avoid this guy, he's a fool. All these people ...</td>\n",
       "      <td>awful</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unhelpful</td>\n",
       "      <td>Avoid this guy, he's a fool. All these people ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>277557.0</td>\n",
       "      <td>This is the best class at UVSC. Don't listen t...</td>\n",
       "      <td>awesome</td>\n",
       "      <td>1</td>\n",
       "      <td>clear</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This is the best class at UVSC. Don't listen t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Professor Lee breaks down complex topics into ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>clear</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Professor Elliot breaks down complex topics in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1281</th>\n",
       "      <td>304408.0</td>\n",
       "      <td>DO NOT TAKE!!! She is a very knowledgeable pro...</td>\n",
       "      <td>average</td>\n",
       "      <td>1</td>\n",
       "      <td>knowledgeable</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DO NOT TAKE!!! She is a very knowledgeable pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>1307460.0</td>\n",
       "      <td>Dr. Parker seems as though he wants you to lea...</td>\n",
       "      <td>poor</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unfair</td>\n",
       "      <td>Dr. Harper seems as though he wants you to lea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1168</th>\n",
       "      <td>NaN</td>\n",
       "      <td>This professor truly cares about the subject m...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>passionate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This professor truly cares about the subject m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>103621.0</td>\n",
       "      <td>This guy is a terrible professor. He is consta...</td>\n",
       "      <td>awful</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>confusing</td>\n",
       "      <td>This guy is a terrible professor. He is consta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>816</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Professor Smith really loves what they teach! ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>passionate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Professor Sidney really loves what they teach!...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1496</th>\n",
       "      <td>NaN</td>\n",
       "      <td>This professor seemed completely disinterested...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unhelpful</td>\n",
       "      <td>This professor seemed completely disinterested...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>158088.0</td>\n",
       "      <td>His curve is so steep, usually a fail on an ex...</td>\n",
       "      <td>awesome</td>\n",
       "      <td>1</td>\n",
       "      <td>supportive</td>\n",
       "      <td>NaN</td>\n",
       "      <td>His curve is so steep, usually a fail on an ex...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4276 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Id                                            comment  quality  \\\n",
       "544    103621.0  Avoid this guy, he's a fool. All these people ...    awful   \n",
       "410    277557.0  This is the best class at UVSC. Don't listen t...  awesome   \n",
       "351         NaN  Professor Lee breaks down complex topics into ...      NaN   \n",
       "1281   304408.0  DO NOT TAKE!!! She is a very knowledgeable pro...  average   \n",
       "598   1307460.0  Dr. Parker seems as though he wants you to lea...     poor   \n",
       "...         ...                                                ...      ...   \n",
       "1168        NaN  This professor truly cares about the subject m...      NaN   \n",
       "466    103621.0  This guy is a terrible professor. He is consta...    awful   \n",
       "816         NaN  Professor Smith really loves what they teach! ...      NaN   \n",
       "1496        NaN  This professor seemed completely disinterested...      NaN   \n",
       "860    158088.0  His curve is so steep, usually a fail on an ex...  awesome   \n",
       "\n",
       "      sentiment positive-attribute negative-attribute  \\\n",
       "544           0                NaN          unhelpful   \n",
       "410           1              clear                NaN   \n",
       "351           1              clear                NaN   \n",
       "1281          1      knowledgeable                NaN   \n",
       "598           0                NaN             unfair   \n",
       "...         ...                ...                ...   \n",
       "1168          1         passionate                NaN   \n",
       "466           0                NaN          confusing   \n",
       "816           1         passionate                NaN   \n",
       "1496          0                NaN          unhelpful   \n",
       "860           1         supportive                NaN   \n",
       "\n",
       "                                     processed_comments  \n",
       "544   Avoid this guy, he's a fool. All these people ...  \n",
       "410   This is the best class at UVSC. Don't listen t...  \n",
       "351   Professor Elliot breaks down complex topics in...  \n",
       "1281  DO NOT TAKE!!! She is a very knowledgeable pro...  \n",
       "598   Dr. Harper seems as though he wants you to lea...  \n",
       "...                                                 ...  \n",
       "1168  This professor truly cares about the subject m...  \n",
       "466   This guy is a terrible professor. He is consta...  \n",
       "816   Professor Sidney really loves what they teach!...  \n",
       "1496  This professor seemed completely disinterested...  \n",
       "860   His curve is so steep, usually a fail on an ex...  \n",
       "\n",
       "[4276 rows x 7 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge and randomize set\n",
    "\n",
    "rd_df = pd.concat([df1, df2])\n",
    "rd_df_sample = rd_df.sample(frac=1, random_state=42) \n",
    "\n",
    "rd_df_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "018de874-6895-47e6-b88e-c57a45045bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Provide an attribute for the following comment. The attribute should one of the following: confusing, unfair, boring, unhelpful, disorganized. Output just the attribute and nothing more. Comment: This is the best class at UVSC. Don't listen to anyone that tells you it's hard -- as long as you can memorize, you'll be fine. There's a ton of material, but nothing is difficult to understand. Emerson & Homan are AWESOME. You DO have to live at school to get an A though, so be ready for 12hr study-sessions!\n"
     ]
    }
   ],
   "source": [
    "rd_df_sample['instruction'] = 'Provide an attribute for the following comment. The attribute should one of the following: confusing, unfair, boring, unhelpful, disorganized. Output just the attribute and nothing more. Comment: '+ rd_df_sample['processed_comments']\n",
    "\n",
    "print(rd_df_sample['instruction'].iloc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ea57fed-4ab2-49f6-8a3a-4a34ebaefdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "\n",
    "### Instruction:\n",
    "\n",
    "{}\n",
    "\n",
    "### Response:\\n\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d4d014a-e8c8-496d-b582-75e7ec3618ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_50584/3371194233.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  rd_df_sample['prompt'] = rd_df_sample[\"instruction\"].apply(\n",
      "/tmp/ipykernel_50584/3371194233.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  rd_df_sample.rename(columns={'negative-attribute': 'response'}, inplace=True)\n",
      "/tmp/ipykernel_50584/3371194233.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  rd_df_sample['response'] = rd_df_sample['response'].astype(str) + \"\\n<|eot_id|>\"\n"
     ]
    }
   ],
   "source": [
    "# Filter out rows where 'negative-attribute' is NaN\n",
    "rd_df_sample = rd_df_sample[rd_df_sample['negative-attribute'].notna()]\n",
    "\n",
    "# Create the prompt format using special tokens\n",
    "rd_df_sample['prompt'] = rd_df_sample[\"instruction\"].apply(\n",
    "    lambda x: f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\"\n",
    "              f\"You are a helpful assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\"\n",
    "              f\"{x}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\n",
    ")\n",
    "\n",
    "# Rename 'negative-attribute' to 'response'\n",
    "rd_df_sample.rename(columns={'negative-attribute': 'response'}, inplace=True)\n",
    "rd_df_sample['response'] = rd_df_sample['response'].astype(str) + \"\\n<|eot_id|>\"\n",
    "\n",
    "# Select only the 'prompt' and 'response' columns\n",
    "rd_df_sample = rd_df_sample[['prompt', 'response']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "151de09a-2dd9-432d-a6ba-e3409a7937bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "You are a helpful assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "Provide an attribute for the following comment. The attribute should one of the following: confusing, unfair, boring, unhelpful, disorganized. Output just the attribute and nothing more. Comment: This class felt like it dragged on forever. Lectures were monotone and rarely strayed from the textbook, making it hard to stay focused. I found myself daydreaming constantly and struggling to absorb the material, even though I sat in the front row.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n"
     ]
    }
   ],
   "source": [
    "print(rd_df_sample['prompt'].iloc[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68df065b-8810-46f6-b3e5-1e8d3ddcd07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_df, test_df = train_test_split(rd_df_sample, test_size=0.07, random_state=42)\n",
    "\n",
    "# Convert DataFrames to Hugging Face Datasets\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "# Create a DatasetDict for easy handling of both sets\n",
    "dataset = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"test\": test_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8139d19b-2aa5-4e1c-8382-57031b27ce81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "/home/jasonspc/course-assn/cs3450/cs3450-team-1-project/ai-backend/train/train-env/lib/python3.12/site-packages/transformers/generation/utils.py:2097: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n",
      "/home/jasonspc/course-assn/cs3450/cs3450-team-1-project/ai-backend/train/train-env/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>### Instruction:\n",
      "Provide an attribute for the following comment. The attribute should one of the following: confusing, unfair, boring, unhelpful, disorganized. Output just the attribute and nothing more. Comment: Professor Smith really hates what they teach!\n",
      "\n",
      "### Response: \n",
      "unfair \n",
      "\n",
      "Please\n"
     ]
    }
   ],
   "source": [
    "prompt= \"\"\"### Instruction:\n",
    "Provide an attribute for the following comment. The attribute should one of the following: confusing, unfair, boring, unhelpful, disorganized. Output just the attribute and nothing more. Comment: Professor Smith really hates what they teach!\n",
    "\n",
    "### Response:\"\"\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "generation_output = model.generate(\n",
    "input_ids=input_ids, max_new_tokens=5\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(generation_output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f3739ed-a8c8-40c8-8093-591583f8cb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_device_train_batch_size = 4\n",
    "gradient_accumulation_steps = 4\n",
    "optim = 'adamw_hf'\n",
    "learning_rate = 1e-5\n",
    "max_grad_norm = 0.3\n",
    "warmup_ratio = 0.03\n",
    "lr_scheduler_type = \"linear\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "207ec14b-6492-47a9-8f29-00ffa39aa4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "temp_output_dir = \"/tmp/training_checkpoints\"\n",
    "os.makedirs(temp_output_dir, exist_ok=True)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=temp_output_dir,\n",
    "    save_strategy=\"steps\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=5,\n",
    "    save_steps=15000000,\n",
    "    logging_steps=5,\n",
    "    num_train_epochs = 3.0,\n",
    "    load_best_model_at_end=True,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    learning_rate=learning_rate,\n",
    "    fp16=True,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f95ebf8b-294a-4a7f-ac55-7b677a9060d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the early stopping callback with patience\n",
    "early_stopping_callback = EarlyStoppingCallback(\n",
    "    early_stopping_patience=3,  # Number of evaluations to wait for improvement\n",
    "    early_stopping_threshold=0.0  # Minimum change to qualify as an improvement\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "43f03d63-4b12-42ff-94db-1be1c6d659c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jasonspc/course-assn/cs3450/cs3450-team-1-project/ai-backend/train/train-env/lib/python3.12/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length. Will not be supported from version '0.13.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/jasonspc/course-assn/cs3450/cs3450-team-1-project/ai-backend/train/train-env/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:300: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/jasonspc/course-assn/cs3450/cs3450-team-1-project/ai-backend/train/train-env/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:328: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5081aae55e984771ab64edb3f8d7987a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1721 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76dee1965247442d9cc94e605790c516",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/130 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset = dataset[\"test\"],\n",
    "    dataset_text_field=\"prompt\",\n",
    "    max_seq_length=150,\n",
    "    args=training_args,\n",
    "    callbacks=[early_stopping_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca6293df-56b2-48c0-8187-3f3ee5e41de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, module in trainer.model.named_modules():\n",
    "  if \"norm\" in name:\n",
    "    module = module.to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "409d42fb-c632-4d55-9095-c52b46a8264f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jasonspc/course-assn/cs3450/cs3450-team-1-project/ai-backend/train/train-env/lib/python3.12/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/home/jasonspc/course-assn/cs3450/cs3450-team-1-project/ai-backend/train/train-env/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/jasonspc/course-assn/cs3450/cs3450-team-1-project/ai-backend/train/train-env/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='321' max='321' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [321/321 32:50, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>4.265300</td>\n",
       "      <td>4.327390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>4.099900</td>\n",
       "      <td>4.288370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>4.012400</td>\n",
       "      <td>4.199097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>4.091100</td>\n",
       "      <td>4.127709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>4.349300</td>\n",
       "      <td>4.030751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>4.122400</td>\n",
       "      <td>3.965452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>3.805500</td>\n",
       "      <td>3.888191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.639800</td>\n",
       "      <td>3.819569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>3.681600</td>\n",
       "      <td>3.742836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.869100</td>\n",
       "      <td>3.667592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>3.909900</td>\n",
       "      <td>3.583606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.466800</td>\n",
       "      <td>3.499457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>3.210200</td>\n",
       "      <td>3.418179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>3.263400</td>\n",
       "      <td>3.337012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>3.290200</td>\n",
       "      <td>3.241795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.516900</td>\n",
       "      <td>3.159106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>3.114400</td>\n",
       "      <td>3.069911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.805900</td>\n",
       "      <td>2.979869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>2.798800</td>\n",
       "      <td>2.898712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.700400</td>\n",
       "      <td>2.815753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>2.951700</td>\n",
       "      <td>2.745777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>3.242000</td>\n",
       "      <td>2.678519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>2.733300</td>\n",
       "      <td>2.614254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>2.455600</td>\n",
       "      <td>2.548100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>2.359700</td>\n",
       "      <td>2.486173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>2.294200</td>\n",
       "      <td>2.427653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>2.547500</td>\n",
       "      <td>2.363656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>2.402200</td>\n",
       "      <td>2.295917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>2.129200</td>\n",
       "      <td>2.231780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.030700</td>\n",
       "      <td>2.171121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>1.988400</td>\n",
       "      <td>2.112051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>2.049200</td>\n",
       "      <td>2.048730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>2.314900</td>\n",
       "      <td>1.999415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.836200</td>\n",
       "      <td>1.950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>1.744100</td>\n",
       "      <td>1.898258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.646600</td>\n",
       "      <td>1.856815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>1.656600</td>\n",
       "      <td>1.821587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>2.018800</td>\n",
       "      <td>1.786028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>1.808600</td>\n",
       "      <td>1.755844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.509000</td>\n",
       "      <td>1.723703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>205</td>\n",
       "      <td>1.563100</td>\n",
       "      <td>1.698461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>1.540300</td>\n",
       "      <td>1.672188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>215</td>\n",
       "      <td>1.796500</td>\n",
       "      <td>1.650025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>1.915700</td>\n",
       "      <td>1.633152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>1.580800</td>\n",
       "      <td>1.613053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>1.443400</td>\n",
       "      <td>1.589351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>235</td>\n",
       "      <td>1.345100</td>\n",
       "      <td>1.567736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.255600</td>\n",
       "      <td>1.554290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>245</td>\n",
       "      <td>1.843600</td>\n",
       "      <td>1.545500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.597200</td>\n",
       "      <td>1.538066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>255</td>\n",
       "      <td>1.413200</td>\n",
       "      <td>1.534608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>1.332100</td>\n",
       "      <td>1.526742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>265</td>\n",
       "      <td>1.352100</td>\n",
       "      <td>1.523466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>1.664500</td>\n",
       "      <td>1.522779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>1.589000</td>\n",
       "      <td>1.512844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>1.339500</td>\n",
       "      <td>1.510996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>285</td>\n",
       "      <td>1.323000</td>\n",
       "      <td>1.510247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>1.351000</td>\n",
       "      <td>1.504603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>295</td>\n",
       "      <td>1.539800</td>\n",
       "      <td>1.499670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.715800</td>\n",
       "      <td>1.503418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>305</td>\n",
       "      <td>1.339900</td>\n",
       "      <td>1.501295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>1.317300</td>\n",
       "      <td>1.501756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>315</td>\n",
       "      <td>1.231000</td>\n",
       "      <td>1.503488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>1.381100</td>\n",
       "      <td>1.498944</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "/home/jasonspc/course-assn/cs3450/cs3450-team-1-project/ai-backend/train/train-env/lib/python3.12/site-packages/peft/utils/save_and_load.py:227: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run(run_name='run'):\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "155955be-5093-4e04-94e4-a4b00ae0c756",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jasonspc/course-assn/cs3450/cs3450-team-1-project/ai-backend/train/train-env/lib/python3.12/site-packages/peft/utils/save_and_load.py:227: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA weights saved to loras/negativeattr-llama3-full\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraModel\n",
    "\n",
    "lora_model = LoraModel(model,peft_config,\"negativeattr-llama3-full\")\n",
    "\n",
    "# Define the directory and naming for saving\n",
    "save_directory = \"loras/negativeattr-llama3-full\"\n",
    "\n",
    "# Save only the LoRA weights\n",
    "lora_model.save_pretrained(save_directory)\n",
    "\n",
    "print(f\"LoRA weights saved to {save_directory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2cfa851d-28d1-4fc5-accf-7da20363fbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.rmtree(temp_output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python train",
   "language": "python",
   "name": "train_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
