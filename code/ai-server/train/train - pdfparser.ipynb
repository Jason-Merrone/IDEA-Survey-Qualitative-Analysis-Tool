{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a07639c-12f5-4c7d-b581-d557696319d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorWithPadding,EarlyStoppingCallback\n",
    "from peft import get_peft_model,get_peft_config, LoraConfig, TaskType, prepare_model_for_kbit_training\n",
    "from datasets import Dataset, DatasetDict\n",
    "import pandas as pd\n",
    "import torch\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from trl import SFTTrainer\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec6fc58e-1b34-47e4-a758-1e030152875f",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN = \"{Enter token here}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fd1b7d4-ed19-4cad-a242-5bfc381f5c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3B-instruct\", padding_side=\"right\", token=TOKEN,)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b3a0ebb-4f0d-4e0f-8c92-4dc29e9c5baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['lm_int8_enable_fp32_cpu_offload']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
     ]
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    lm_int8_enable_fp32_cpu_offload=True,\n",
    "    llm_int8_skip_modules=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c015b75-ebb0-4248-a416-6ca933628142",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b85b9c643ccf4ef386b3c764f9b8970d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-3.2-3B-instruct\",\n",
    "    quantization_config=bnb_config,\n",
    "    token=TOKEN,\n",
    "    device_map='auto',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e436d15f-168d-4b01-81ab-a7a4d47f0b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69f7af1e-ec58-4567-bbc7-7fca9982f18c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 16,509,440 || all params: 3,229,259,264 || trainable%: 0.5112\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jasonspc/course-assn/cs3450/cs3450-team-1-project/ai-backend/train/train-env/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:500: UserWarning: Model with `tie_word_embeddings=True` and the tied_target_modules=['lm_head'] are part of the adapter. This can lead to complications, for example when merging the adapter or converting your model to formats other than safetensors. See for example https://github.com/huggingface/peft/issues/2018.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "target_modules = ['q_proj','k_proj','v_proj','o_proj','gate_proj','down_proj','up_proj','lm_head']\n",
    "# PEFT Configuration\n",
    "peft_config = LoraConfig(\n",
    "    r=10,\n",
    "    target_modules = target_modules,\n",
    "    lora_alpha=8,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)\n",
    "print(model.print_trainable_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8beddcc4-c60b-47da-91ed-196129c25c15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw-comments</th>\n",
       "      <th>processed-comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1447</th>\n",
       "      <td>Professor Smith's lectures were incredibly den...</td>\n",
       "      <td>|&lt;startofcomment&gt;| Professor Smith's lectures ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1114</th>\n",
       "      <td>He might not be the clearest lecturer in the w...</td>\n",
       "      <td>|&lt;startofcomment&gt;| He might not be the cleares...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1064</th>\n",
       "      <td>I would not recommend this professor. I really...</td>\n",
       "      <td>|&lt;startofcomment&gt;| I would not recommend this ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2287</th>\n",
       "      <td>This professor truly brought the material to l...</td>\n",
       "      <td>|&lt;startofcomment&gt;| This professor truly brough...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1537</th>\n",
       "      <td>His lectures jump around a lot, and it's hard ...</td>\n",
       "      <td>|&lt;startofcomment&gt;| His lectures jump around a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1638</th>\n",
       "      <td>Lectures and are really long and boring. Test ...</td>\n",
       "      <td>|&lt;startofcomment&gt;| Lectures and are really lon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1095</th>\n",
       "      <td>Sidney is incredible smart and knows statistic...</td>\n",
       "      <td>|&lt;startofcomment&gt;| Sidney is incredible smart ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1130</th>\n",
       "      <td>This class felt incredibly chaotic. The syllab...</td>\n",
       "      <td>|&lt;startofcomment&gt;| This class felt incredibly ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1294</th>\n",
       "      <td>Professor Elliot breaks down complex topics in...</td>\n",
       "      <td>|&lt;startofcomment&gt;| Professor Elliot breaks dow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>This class was incredibly frustrating. Profess...</td>\n",
       "      <td>|&lt;startofcomment&gt;| This class was incredibly f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2500 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           raw-comments  \\\n",
       "1447  Professor Smith's lectures were incredibly den...   \n",
       "1114  He might not be the clearest lecturer in the w...   \n",
       "1064  I would not recommend this professor. I really...   \n",
       "2287  This professor truly brought the material to l...   \n",
       "1537  His lectures jump around a lot, and it's hard ...   \n",
       "...                                                 ...   \n",
       "1638  Lectures and are really long and boring. Test ...   \n",
       "1095  Sidney is incredible smart and knows statistic...   \n",
       "1130  This class felt incredibly chaotic. The syllab...   \n",
       "1294  Professor Elliot breaks down complex topics in...   \n",
       "860   This class was incredibly frustrating. Profess...   \n",
       "\n",
       "                                     processed-comments  \n",
       "1447  |<startofcomment>| Professor Smith's lectures ...  \n",
       "1114  |<startofcomment>| He might not be the cleares...  \n",
       "1064  |<startofcomment>| I would not recommend this ...  \n",
       "2287  |<startofcomment>| This professor truly brough...  \n",
       "1537  |<startofcomment>| His lectures jump around a ...  \n",
       "...                                                 ...  \n",
       "1638  |<startofcomment>| Lectures and are really lon...  \n",
       "1095  |<startofcomment>| Sidney is incredible smart ...  \n",
       "1130  |<startofcomment>| This class felt incredibly ...  \n",
       "1294  |<startofcomment>| Professor Elliot breaks dow...  \n",
       "860   |<startofcomment>| This class was incredibly f...  \n",
       "\n",
       "[2500 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and prepare datasets\n",
    "df = pd.read_csv(\"dataset/pdf-parse-dataset.csv\")\n",
    "\n",
    "rd_df_sample = df.sample(frac=1, random_state=42) \n",
    "rd_df_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5aa69529-532f-4ee2-bcec-0cab57a2c36a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please parse the following comments. Do so by placing the token |<startofcomment>| at the start of a comment and the token |<endofcomment>| at the end of one. Output just this and nothing more. Comments to process: He might not be the clearest lecturer in the world but he is the absolute BEST one-on-one teacher I had in my four years at UND. He's so encouraging and genuinely wants his students to do well. If you're not willing to spend time on the homework & go to his office hours then you probably won't get a good grade, but that's your own fault. Difficult but here are my suggestions: Attend ALL lectures&fill out his course packet. Be polite to him. Ask for help after class and in office hours. Write down all your work for online HW even if you aren't doing the notebook. Do the ENTIRE study guide and practice test to study for exams. If can do those comfortably you're set. Orgo is intrinsically a difficult course, but Taylor did a good job explaining the harder concepts. Of course it took a lot of time to understand, but I learned a lot. To those complaining about the difficulty of his lectures and exams, isn't that what you expected? Don't complain about something that was told to you from the start. This class felt like a constant scramble. The syllabus changed multiple times throughout the semester, and sometimes we'd get assignments announced in class with no prior notice. It was hard to keep track of what was due when and which readings were actually important. I spent way more time trying to figure out what was going on than actually learning the material. This professor lays everything out perfectly from day one. You always know what to expect and what they are looking for in assignments. I never felt lost or confused in this class! This class was incredibly difficult. I went to office hours multiple times, but I still struggled to understand the material. Even when I asked specific questions, it felt like I wasn't getting clear answers. The lectures often left me with more questions than answers. He is very nice and willing to take time to help any of students. Super smart. This professor truly loves what they teach! Every lecture felt like a fascinating journey into the subject matter. They have a contagious enthusiasm that makes even complex topics feel exciting and accessible. Be prepared to participate - their classes are dynamic and discussion-driven, which is fantastic for really understanding the material. Majors orgo wasn't too bad. Most of the learning takes place during the group problems with the TAs. The lectures are pretty tough to understand but reading ahead definitely helps Professor Sidney is amazing! They really care about their students understanding the material. They are always willing to answer questions, even after class. You can tell they genuinely want everyone to succeed. I would definitely recommend taking a class with them. He plays favorites. It felt like he only cared about certain students succeeding, and I never knew where I stood. My grades were inconsistent, even when I thought I was doing well on assignments. There seemed to be no clear grading criteria and it felt impossible to get a straight answer from him. he's so smart it's scary. grading was definitely unfair. it's so interesting though it's a shame to not take it but you won't get an A Professor Xâs lectures often jumped between topics without clear transitions, making it hard to follow the main points. While they clearly have a deep knowledge of the subject, explaining complex ideas in a way that was easy for students to grasp proved challenging. It felt like I spent more time trying to piece together what they were saying than actually learning the material. Professor Blaine is a phenomenal teacher! He breaks down complex concepts into manageable pieces and always provides relevant examples that make the material easy to grasp. I never felt lost in his class, and I always left feeling like I had a strong understanding of the subject matter. Terrible teacher all around. Had him my first year in college for math and got stuck with him again my junior year and nothing changed.. he maybe even got worse.. She's very intelligent and has a good grasp on the subject and makes class interesting. I definitely would have to say she's a great professor thus far. She's also very cute and has beautiful eyes. For you recent highschoolers: Make sure you remember this is college and treat this class as such... I don't think she tolerates kiddish behavior.AN friggin, hated, this class. i was an econ major, and after taking this class, i lost all my interest in it. im changing my major, because of this guy. probably the most ridiculous/inefficient grading system EVER INVENTED. studied my ass off for all the tests, ended up with a friggin B. pompous, condescending, unfriendly man. AVOID HIM This class felt incredibly chaotic. It was hard to tell what was important because the lectures jumped around so much, and sometimes we wouldn't even cover the assigned readings. I felt like I was constantly trying to catch up and figure out what was going on. This professor seemed to change their grading criteria constantly, making it impossible to know what they actually wanted. One minute a certain type of answer was fine, the next it was wrong. I studied hard for this class and still felt completely lost due to the lack of clear expectations. This professor seems to have favorites. If you ask a question he doesn't like, be prepared for him to make you feel stupid in front of the whole class. Grades are all over the place and it feels impossible to figure out what he actually wants from us. Wouldn't recommend. I loved his class! He was so funny and notes really helped for the test he gave which anyone can pass if they study the book and notes he gave in class. Also made sure you got your experiment credits done. Was a great help! Had alot to expound upon, does get behind and then rushes through to get caught up test were tricky sometimes but was fair. Can't complain w/ an A- This professor truly loves what they teach! Every lecture felt like a fascinating journey of discovery, and their enthusiasm was contagious. You can tell they really care about sharing their knowledge and making sure students understand the material. I always left class feeling inspired to learn more. River is a nice guy. The problem is what he goes over in class and then what the assignments look like are two different things. If you don't want to be frustrated and feeling lost all the time avoid him at all cost's Most people come to this site because they are only interested in getting the easiest professor. They don't care about what they're or they are too stupid to understand anything. Prof. Bailey is a good professor but you have to study. The textbook he uses is very difficult to follow and is completely different than the Aplia homework but it is easier to follow than his lectures. Mackenzie loves economics and knows it very well that he assumes that an intro class should be easy. Midterms are ridiculously hard, probably 10x the hw. Average midterm grade was a 53%, take a diff prof \n"
     ]
    }
   ],
   "source": [
    "rd_df_sample['instruction'] = 'Please parse the following comments. Do so by placing the token |<startofcomment>| at the start of a comment and the token |<endofcomment>| at the end of one. Output just this and nothing more. Comments to process: '+ rd_df_sample['raw-comments']\n",
    "\n",
    "print(rd_df_sample['instruction'].iloc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27677cf1-0ccc-4ad6-9fd1-c992d007b9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "\n",
    "### Instruction:\n",
    "\n",
    "{}\n",
    "\n",
    "### Response:\\n\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a39b03b-62cb-4a4d-9056-e4288756af60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the prompt format using special tokens\n",
    "rd_df_sample['prompt'] = rd_df_sample[\"instruction\"].apply(\n",
    "    lambda x: f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\"\n",
    "              f\"You are a helpful assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\"\n",
    "              f\"{x}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\n",
    ")\n",
    "\n",
    "# Rename 'positive-attribute' to 'response'\n",
    "rd_df_sample.rename(columns={'processed-comments': 'response'}, inplace=True)\n",
    "rd_df_sample['response'] = rd_df_sample['response'].astype(str) + \"\\n<|eot_id|>\"\n",
    "\n",
    "# Select only the 'prompt' and 'response' columns\n",
    "rd_df_sample = rd_df_sample[['prompt', 'response']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74e15ef0-863c-4ec8-8d49-6b78baed9d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_df, test_df = train_test_split(rd_df_sample, test_size=0.07, random_state=42)\n",
    "\n",
    "# Convert DataFrames to Hugging Face Datasets\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "# Create a DatasetDict for easy handling of both sets\n",
    "dataset = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"test\": test_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27ddc801-c255-407c-b6dd-8107bf77fdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_device_train_batch_size = 4\n",
    "gradient_accumulation_steps = 4\n",
    "optim = 'adamw_hf'\n",
    "learning_rate = 1e-5\n",
    "max_grad_norm = 0.3\n",
    "warmup_ratio = 0.03\n",
    "lr_scheduler_type = \"linear\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9ff39ec-ec13-4e5f-950a-e4879d8ff663",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "temp_output_dir = \"/tmp/training_checkpoints\"\n",
    "os.makedirs(temp_output_dir, exist_ok=True)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=temp_output_dir,\n",
    "    save_strategy=\"steps\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=5,\n",
    "    save_steps=15000000,\n",
    "    logging_steps=5,\n",
    "    num_train_epochs = 3.0,\n",
    "    load_best_model_at_end=True,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    learning_rate=learning_rate,\n",
    "    fp16=True,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "006933e9-ace5-46e6-b8b4-884efbe3e096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the early stopping callback with patience\n",
    "early_stopping_callback = EarlyStoppingCallback(\n",
    "    early_stopping_patience=3,  # Number of evaluations to wait for improvement\n",
    "    early_stopping_threshold=0.0  # Minimum change to qualify as an improvement\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "354fe80a-fbbc-4883-9e35-83242de98060",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jasonspc/course-assn/cs3450/cs3450-team-1-project/ai-backend/train/train-env/lib/python3.12/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length. Will not be supported from version '0.13.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/jasonspc/course-assn/cs3450/cs3450-team-1-project/ai-backend/train/train-env/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:300: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/jasonspc/course-assn/cs3450/cs3450-team-1-project/ai-backend/train/train-env/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:328: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38f23205b1b34f54ade2f97d8ac6e6a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2324 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aa4921340be4ccc9575ef2946cb1313",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/176 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset = dataset[\"test\"],\n",
    "    dataset_text_field=\"prompt\",\n",
    "    max_seq_length=350,\n",
    "    args=training_args,\n",
    "    callbacks=[early_stopping_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c07699ff-bb48-47f5-bac1-585dfb521e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, module in trainer.model.named_modules():\n",
    "  if \"norm\" in name:\n",
    "    module = module.to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "17b6bb20-0b01-4d19-a4ea-f0b05e43f38a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jasonspc/course-assn/cs3450/cs3450-team-1-project/ai-backend/train/train-env/lib/python3.12/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/home/jasonspc/course-assn/cs3450/cs3450-team-1-project/ai-backend/train/train-env/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/jasonspc/course-assn/cs3450/cs3450-team-1-project/ai-backend/train/train-env/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='435' max='435' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [435/435 1:52:40, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.561000</td>\n",
       "      <td>3.617306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.474400</td>\n",
       "      <td>3.603386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>3.497400</td>\n",
       "      <td>3.566506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.438300</td>\n",
       "      <td>3.521774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>3.395400</td>\n",
       "      <td>3.473594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.320700</td>\n",
       "      <td>3.429173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>3.620200</td>\n",
       "      <td>3.386432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.849400</td>\n",
       "      <td>3.336605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>3.209800</td>\n",
       "      <td>3.292722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.174000</td>\n",
       "      <td>3.244813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>3.130500</td>\n",
       "      <td>3.195901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.131800</td>\n",
       "      <td>3.158282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>3.053700</td>\n",
       "      <td>3.122806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>3.180600</td>\n",
       "      <td>3.073998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>3.674300</td>\n",
       "      <td>3.033469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.919700</td>\n",
       "      <td>2.996727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>2.904200</td>\n",
       "      <td>2.943574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.836900</td>\n",
       "      <td>2.902705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>2.817700</td>\n",
       "      <td>2.861484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.796100</td>\n",
       "      <td>2.815621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>2.757900</td>\n",
       "      <td>2.782700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>3.001600</td>\n",
       "      <td>2.732994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>2.679900</td>\n",
       "      <td>2.697005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>2.592800</td>\n",
       "      <td>2.661781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>2.662900</td>\n",
       "      <td>2.627096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>2.608700</td>\n",
       "      <td>2.591699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>2.568400</td>\n",
       "      <td>2.558687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>2.511500</td>\n",
       "      <td>2.528171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>2.366400</td>\n",
       "      <td>2.493255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.810000</td>\n",
       "      <td>2.460224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>2.458700</td>\n",
       "      <td>2.429350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>2.346100</td>\n",
       "      <td>2.401416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>2.430400</td>\n",
       "      <td>2.374490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>2.342800</td>\n",
       "      <td>2.340870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>2.283200</td>\n",
       "      <td>2.321570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>2.082000</td>\n",
       "      <td>2.288943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>2.358100</td>\n",
       "      <td>2.261352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>2.265900</td>\n",
       "      <td>2.231789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>2.212400</td>\n",
       "      <td>2.223045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.196500</td>\n",
       "      <td>2.205686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>205</td>\n",
       "      <td>2.234100</td>\n",
       "      <td>2.190157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>2.157300</td>\n",
       "      <td>2.167031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>215</td>\n",
       "      <td>2.038400</td>\n",
       "      <td>2.166097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>2.045000</td>\n",
       "      <td>2.154934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>2.137800</td>\n",
       "      <td>2.137746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>2.170200</td>\n",
       "      <td>2.132993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>235</td>\n",
       "      <td>2.162500</td>\n",
       "      <td>2.127141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>2.169600</td>\n",
       "      <td>2.117492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>245</td>\n",
       "      <td>2.130700</td>\n",
       "      <td>2.112173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>2.028800</td>\n",
       "      <td>2.104929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>255</td>\n",
       "      <td>1.885300</td>\n",
       "      <td>2.100191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>2.112700</td>\n",
       "      <td>2.100681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>265</td>\n",
       "      <td>2.156700</td>\n",
       "      <td>2.092101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>2.159600</td>\n",
       "      <td>2.089254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>2.145700</td>\n",
       "      <td>2.087066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>2.072400</td>\n",
       "      <td>2.079232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>285</td>\n",
       "      <td>2.047900</td>\n",
       "      <td>2.079298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>1.819300</td>\n",
       "      <td>2.070776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>295</td>\n",
       "      <td>2.300400</td>\n",
       "      <td>2.061562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.081100</td>\n",
       "      <td>2.072454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>305</td>\n",
       "      <td>2.085800</td>\n",
       "      <td>2.072770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>2.131000</td>\n",
       "      <td>2.059094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>315</td>\n",
       "      <td>2.130700</td>\n",
       "      <td>2.055562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>2.049000</td>\n",
       "      <td>2.060187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>1.840700</td>\n",
       "      <td>2.054446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>1.919100</td>\n",
       "      <td>2.057596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>335</td>\n",
       "      <td>2.128400</td>\n",
       "      <td>2.054107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>2.018500</td>\n",
       "      <td>2.051625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>345</td>\n",
       "      <td>2.045300</td>\n",
       "      <td>2.050430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>2.024900</td>\n",
       "      <td>2.041185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>355</td>\n",
       "      <td>2.034500</td>\n",
       "      <td>2.052029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>1.965000</td>\n",
       "      <td>2.043644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>365</td>\n",
       "      <td>1.828600</td>\n",
       "      <td>2.044325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>2.091000</td>\n",
       "      <td>2.040236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>2.099900</td>\n",
       "      <td>2.045420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>2.060000</td>\n",
       "      <td>2.038063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>385</td>\n",
       "      <td>2.004300</td>\n",
       "      <td>2.044742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>1.977000</td>\n",
       "      <td>2.034165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>395</td>\n",
       "      <td>1.993800</td>\n",
       "      <td>2.039673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.731700</td>\n",
       "      <td>2.036192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>405</td>\n",
       "      <td>2.130300</td>\n",
       "      <td>2.040780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>2.068600</td>\n",
       "      <td>2.031344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>415</td>\n",
       "      <td>2.079300</td>\n",
       "      <td>2.035881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>2.038400</td>\n",
       "      <td>2.036935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>2.056800</td>\n",
       "      <td>2.037359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>2.026700</td>\n",
       "      <td>2.036924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>435</td>\n",
       "      <td>1.562100</td>\n",
       "      <td>2.037237</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "/home/jasonspc/course-assn/cs3450/cs3450-team-1-project/ai-backend/train/train-env/lib/python3.12/site-packages/peft/utils/save_and_load.py:227: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run(run_name='run'):\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f422356-ff24-4893-a55d-8aff501b7529",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jasonspc/course-assn/cs3450/cs3450-team-1-project/ai-backend/train/train-env/lib/python3.12/site-packages/peft/utils/save_and_load.py:227: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA weights saved to loras/pdfparser-llama3-full\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraModel\n",
    "\n",
    "lora_model = LoraModel(model,peft_config,\"pdfparser-llama3-full\")\n",
    "\n",
    "# Define the directory and naming for saving\n",
    "save_directory = \"loras/pdfparser-llama3-full\"\n",
    "\n",
    "# Save only the LoRA weights\n",
    "lora_model.save_pretrained(save_directory)\n",
    "\n",
    "print(f\"LoRA weights saved to {save_directory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2f65812f-6a69-4e9c-a445-47a4332e60a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.rmtree(temp_output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python train",
   "language": "python",
   "name": "train_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
